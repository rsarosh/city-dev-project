{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Behind most websites is some sort of data structure.\n",
    "\n",
    "When you see something like a list of search results from Google or a list of items on Amazon, there is a structure to the data that makes up the list.\n",
    "\n",
    "For example, for just 1 Google search result, these variables about the search result are visible:\n",
    "* The title of the page that the link goes to\n",
    "* The URL of the page that the link goes to\n",
    "* A snippet of some of the text on the page that the link goes to\n",
    "\n",
    "This same format is followed for all of the other search results that all have that same information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Packages\n",
    "\n",
    "Web scraping is used to extract data such as the above.\n",
    "\n",
    "There are a number of packages that can assist this, and it can be done in many languages.\n",
    "\n",
    "In Python there are packages that you can install that provide a lot of different functionalities. You should only have to install each package once.\n",
    "\n",
    "For webscraping, we will use 2 packages: Requests & BeautifulSoup4\n",
    "\n",
    "* Requests is a package that lets you request and get the raw HTML from webpages: http://docs.python-requests.org/en/master/\n",
    "\n",
    "* BeautifulSoup4 is a package used to assist in webscraping: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "To install packages, run the code below. (This can also be run in a terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the \"requests\" and \"BeautifulSoup\" packages\n",
    "!pip install requests\n",
    "!pip install BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages\n",
    "\n",
    "To make the functionality of each package available, you will need to import the installed package.\n",
    "\n",
    "If errors appear when running the code below, then the package wasn't installed correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "print('Done importing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping\n",
    "\n",
    "Now, let's scrape data!\n",
    "\n",
    "The site we will be scraping data from is IDTech - https://www.idtech.com/courses\n",
    "\n",
    "On the Courses page, there is one row for every course, each of which follow the same format.\n",
    "\n",
    "We will get information about every course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Setting things up\n",
    "\n",
    "The links on this website use relative URLs, so saving the base URL is very helpful.\n",
    "\n",
    "We will also make an empty list which will contain dictionaries to convert into a CSV file at the end. Each dictionary in this list will become 1 row and contain the above information (Title, URL, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base url of the website. Let's set this once so we won't have to set it again.\n",
    "base_url = 'https://www.idtech.com'\n",
    "\n",
    "csv = [] # Empty list to contain dictionaries that will become a CSV at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Grabbing the raw HTML of the course directory page\n",
    "\n",
    "On the course directory page, you can access the page of every course that's available: https://www.idtech.com/courses\n",
    "\n",
    "So we will start there and get all the raw HTML data.\n",
    "\n",
    "From there, we can get all the links to all the different courses, then loop over them to extract each one's data, turn that data into a dictionary, and create a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a request to the Courses page and turn it into raw HTML using BeautifulSoup\n",
    "r = requests.get(base_url + '/courses')\n",
    "soup = BeautifulSoup(r.text, 'html5lib')\n",
    "\n",
    "# You can check out the raw HTML by printing soup\n",
    "#print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of trying to read the printed soup, I would recommend just going to the webpage you are about to scrape, right click the page and press \"Inspect\", then Elements will have the same content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Grabbing the links to each individual course page\n",
    "\n",
    "In a browser like Chrome or Firefox, with the inspector open, click on the top left option that says \"Select an element in the page to inspect it\"\n",
    "\n",
    "Then click the link to each page, such as \"Code-a-bot: AI and Robotics with Your Own Cozmo\".\n",
    "\n",
    "Notice that it has the class \"course-name\". When you search for \"course-name\" in the inspector, you will find that everything with this class is a link to a course.\n",
    "\n",
    "So, we will want to get only the links with the class \"course-name\", then get each one's \"href\", which is the URL of each page, relative to the base URL.\n",
    "\n",
    "BeautifulSoup helps us in doing this with the find_all function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all tags in the soup that are <a> (anchor/link) tags that have the class \"course-name\"\n",
    "course_tags = soup.find_all('a', attrs={'class': 'course-name'})\n",
    "\n",
    "#print(course_tags)\n",
    "print(len(course_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Looping through all URLs of course pages\n",
    "\n",
    "With each course tag, we can get its href and concatenate it to the end of the base URL to get the URL to every course page.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_urls = []\n",
    "\n",
    "# Loop over every course tag content\n",
    "for course_tag in course_tags:\n",
    "    \n",
    "    # Assemble the URL of the specific course page:\n",
    "    course_url = base_url + course_tag['href'] # Access the \"href\" of every <a> course tag\n",
    "    course_urls.append(course_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Getting the data!\n",
    "\n",
    "Now that we have a list of URLs for each page, we can request the raw HTML of each page to be able to find the information we want.\n",
    "\n",
    "### For every course, we want the following information:\n",
    "1. #### __URL__: The URL of the course\n",
    "\n",
    "* #### __Title__: The title of the course\n",
    "\n",
    "* #### __Topic__: The title is NOT the topic. The topic for example could be something like \"Robotics\", \"Design\", or \"Coding\", but under this topic, there could be many courses with different titles.\n",
    "\n",
    "* #### __SkillLevel__: The skill requirement of the course, such as \"Beginner\" or \"Advanced\"\n",
    "\n",
    "* #### __CourseType__: There are several types of courses, such as Summer Camps or just regular classes. We want to distinguish between each of these.\n",
    "\n",
    "* #### __MinimumAge__: The minimum age that someone can enroll in the course at. If there is no minimum, set this to be -1.\n",
    "\n",
    "* #### __MaximumAge__: The maximum age that someone can enroll in the course at. If there is no maximum, set this to be -1.\n",
    "\n",
    "* #### __Description__: A description of the course - course pages will usually have a description of what the course is about.\n",
    "\n",
    "* #### __Provider__: The main title of the website itself, such as \"ID Tech\" or \"Computing Kids\". This will be the same for every course per website - all courses scraped from ID Tech will have \"ID Tech\" as the provider.\n",
    "\n",
    "* #### __Locations__: The locations where the course takes place. Multiple locations are in one string, so make sure you set this to be a list []. This list will be processed to remove duplicates later.\n",
    "\n",
    "\n",
    "So we must find where in the page each field is located and get its contents using BeautifulSoup's functions.\n",
    "\n",
    "Each course page will have its own dictionary that contains all of the fields, then get added to the list \"csv\" that we created in Step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = []\n",
    "\n",
    "print('Start')\n",
    "\n",
    "# Get every course page's information and put it in the \"csv\" list of dictionaries\n",
    "for course_url in course_urls:\n",
    "    \n",
    "    # New course dictionary, will get added to \"csv\" list\n",
    "    course = {} \n",
    "\n",
    "    #__________\n",
    "    # Get the content of the course page\n",
    "    r = requests.get(course_url)\n",
    "    soup = BeautifulSoup(r.text, 'html5lib')\n",
    "\n",
    "    #__________\n",
    "    # [1] URL\n",
    "    #       The URL that we are scraping from\n",
    "    \n",
    "    course['URL'] = course_url\n",
    "    \n",
    "    #__________\n",
    "    # [2] Title\n",
    "    #       The first h1 tag has the title.\n",
    "    \n",
    "    title = soup.find('h1').getText()\n",
    "    course['Title'] = title\n",
    "\n",
    "    # Age, skill level, type and age are all in this table\n",
    "    course_attributes_blocks = soup.find('div', attrs={'class': 'course-attributes'})\n",
    "    col_sm_11 = course_attributes_blocks.find('dl', attrs={'class': 'col-sm-6'})\n",
    "    dds = col_sm_11.find_all('dd')\n",
    "\n",
    "    #__________\n",
    "    # [3] Topic\n",
    "    #        The 3rd <dd> in the first <dl> of <div> with class \"course-attributes\"\n",
    "    topic = dds[2].getText()\n",
    "    course['Topic'] = topic\n",
    "\n",
    "    #__________\n",
    "    # [4] SkillLevel\n",
    "    #        The 2nd <dd> in the first <dl> of <div> with class \"course-attributes\"\n",
    "    skill_level = dds[1].getText()\n",
    "    skill_level = skill_level.replace('–', '-')\n",
    "    course['SkillLevel'] = skill_level\n",
    "\n",
    "    #__________\n",
    "    # [5] CourseType\n",
    "    #        The 1st <dd> in the 2nd <dl> of <div> with class \"course-attributes\"\n",
    "    col_sm_ll2 = course_attributes_blocks.find_all('dl', attrs={'class': 'col-sm-6'})[1]\n",
    "    dds_2 = col_sm_ll2.find_all('dd')\n",
    "    course_type = dds_2[0].getText()\n",
    "    \n",
    "    course['CourseType'] = course_type\n",
    "\n",
    "    #__________\n",
    "    # [6] MinimumAge\n",
    "    #        The age range is from the 1st <dd> in the first <dl> of <div> with class \"course-attributes\"\n",
    "    age_range = dds[0].getText().replace('-', '').split(' ')\n",
    "    \n",
    "    # The age range can be split into the lower age, and the higher age.\n",
    "    course['MinimumAge'] = age_range[0]\n",
    "    \n",
    "    #__________\n",
    "    # [7] MaximumAge\n",
    "    course['MaximumAge'] = age_range[2]\n",
    "\n",
    "    #__________\n",
    "    # [8] Description\n",
    "    #        The content of a div with class \"content\" inside a div with the class \"course-details\"\n",
    "    course_details = soup.find('div', attrs={'class': 'course-details'})\n",
    "    course_details_content = course_details.find('div', attrs={'class': 'content'})\n",
    "    \n",
    "    course['Description'] = course_details_content.getText()\n",
    "    \n",
    "    #__________\n",
    "    # [9] Provider\n",
    "    #       The name of the website. This is very important for distinguishing different providers\n",
    "    course['Provider'] = 'iDTech'\n",
    "    \n",
    "    #__________\n",
    "    # [10] Locations\n",
    "    collapsed_lis = soup.find_all('li', attrs={'class': 'collapsed'})\n",
    "    \n",
    "    locations = []\n",
    "    \n",
    "    # Get the address of every location available\n",
    "    for li in collapsed_lis:\n",
    "        uls = li.find_all('ul', attrs={'class': 'location-list'})\n",
    "        \n",
    "        for ul in uls:\n",
    "            location_lis = ul.find_all('li')\n",
    "            \n",
    "            for location_li in location_lis:\n",
    "                address = location_li.find('span', attrs={'class': 'address'}).getText()\n",
    "                locations.append(address)\n",
    "                \n",
    "    course['Locations'] = locations\n",
    "    \n",
    "\n",
    "    csv.append(course)\n",
    "\n",
    "#________________\n",
    "# Finally, convert list of dictionaries into a CSV\n",
    "import pandas as pd\n",
    "\n",
    "# Change list of dictionaries of courses into a dataframe\n",
    "df = pd.DataFrame(csv)\n",
    "\n",
    "# Save dataframe as a CSV\n",
    "df.to_csv('course_idtech.csv', index=False,encoding='utf-8')\n",
    "\n",
    "print('Complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "After you run all of the code above, on the left you should see a \"course_idtech.csv\" file, which you can open here, or in Excel.\n",
    "\n",
    "When you scrape websites the above process will have to get information from completely different places, since different websites have different structures.\n",
    "\n",
    "Some of the information may not be available at all either, so if you can't find the data, make note of this.\n",
    "\n",
    "Every site can be vastly different than all others, so a different scraping script will be needed for different websites.\n",
    "\n",
    "The more entries there are on a site, the longer the script will take to finish. However, sites with more courses may actually be easier to scrape than websites with fewer courses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What will be doing with all these CSV files?\n",
    "\n",
    "We are making a website that aggregrates many \"providers\" throughout Washington as well as the courses that they offer.\n",
    "\n",
    "Example of Providers include iDTech, the Pacific Science Center, or Seattle .gov, basically anything that has technical classes available.\n",
    "\n",
    "Every one of these Providers will have their information scraped with the fields specified above.\n",
    "\n",
    "With further adjustments such as grouping certain topics together (one provider may give a topic a different name than a different provider) that will occur after all the data is scraped, the CSV files will all be combined into one CSV file and be used to create a database table for our site, \"Digital Skills For All\"\n",
    "\n",
    "Then users of our site can search for parameters they want, such as certain skills or skill level or recommended age, and the sufficient results will appear across all of these different providers.\n",
    "\n",
    "If you have any questions, email me at greycabbage@gmail.com\n",
    "\n",
    "Here is a list of providers:\n",
    "1. https://www.pacificsciencecenter.org/\n",
    "* http://sdkbridge.com/youth.php\n",
    "* http://www.piercecountylibrary.org/\n",
    "* https://codeday.org/\n",
    "* http://www.techsmartkids.com/\n",
    "* https://www.tbcs.org/community/summer-camp\n",
    "* http://www.bigbrainedsuperheroes.org/\n",
    "* https://www.cs.washington.edu/outreach/k12\n",
    "* https://www.summer-camp.uw.edu/\n",
    "* https://www.thecoderschool.com/bellevue?tcs=\n",
    "* https://www.idtech.com/courses (already done)\n",
    "* http://apprenticareers.org/apply/\n",
    "* https://www.literacysource.org/programs/\n",
    "* http://www.elcentrodelaraza.org/events/event/\n",
    "* https://sahareducation.org/digital-literacy/\n",
    "* http://www.seattle.gov/iandraffairs/RTW\n",
    "* https://seattlegoodwill.org/job-training-and-education/work-readiness/computer-classes\n",
    "* https://www.seattleymca.org/accelerator/ytech\n",
    "* https://www.spl.org/programs-and-services/learning/learning-calendar\n",
    "* https://www.nhseattle.com/about-us/contact-us\n",
    "* https://generalassemb.ly/seattle/marketing/digital-marketing\n",
    "* https://itconnect.uw.edu/learn/workshops/\n",
    "* https://itconnect.uw.edu/learn/workshops/online-tutorials/\n",
    "* https://www.intrepidlearning.com/training-providers\n",
    "* http://seattlecolleges.edu/\n",
    "* https://www.onlc.com/dirwase.htm\n",
    "* https://www.techsherpas.com/it-training-centers/seattle-wa/\n",
    "* https://www.ramco-training.com/index.html\n",
    "* https://www.pryor.com/\n",
    "* https://generalassemb.ly/locations/seattle\n",
    "* https://learncodinganywhere.com/TheTechAcademySeattle\n",
    "* https://ncs.seattleu.edu/programs-courses/digital-technology/\n",
    "* https://rtc.edu/#\n",
    "* http://www.kalacademy.org/\n",
    "* www.schoolsoutwashington.org\n",
    "* https://www.501commons.org/services/technology-services/plan-IT-program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
